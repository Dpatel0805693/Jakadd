================================================================================
KHALIL'S CORE API BACKEND - ALL FILES
================================================================================

Instructions:
1. Copy each section below into the corresponding file
2. Create the directory structure as shown
3. Follow the setup instructions at the end

Directory structure to create:
backend/
â”œâ”€â”€ server.js
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ User.js
â”‚   â”œâ”€â”€ File.js
â”‚   â””â”€â”€ Analysis.js
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ authRoutes.js
â”‚   â”œâ”€â”€ fileRoutes.js
â”‚   â”œâ”€â”€ analysisRoutes.js
â”‚   â”œâ”€â”€ resultRoutes.js
â”‚   â”œâ”€â”€ glossaryRoutes.js
â”‚   â””â”€â”€ healthRoutes.js
â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ auth.js
â”‚   â”œâ”€â”€ upload.js
â”‚   â””â”€â”€ errorHandler.js
â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ package.json
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env.example
â””â”€â”€ .gitignore

================================================================================
FILE: backend/server.js
================================================================================

// server.js - Khalil's Core API (Port 3000)
// Main entry point for all client requests

import express from "express";
import mongoose from "mongoose";
import cors from "cors";
import dotenv from "dotenv";
import morgan from "morgan";

// Import routes
import authRoutes from "./routes/authRoutes.js";
import fileRoutes from "./routes/fileRoutes.js";
import analysisRoutes from "./routes/analysisRoutes.js";
import resultRoutes from "./routes/resultRoutes.js";
import glossaryRoutes from "./routes/glossaryRoutes.js";
import healthRoutes from "./routes/healthRoutes.js";

// Import error handler
import errorHandler from "./middleware/errorHandler.js";

dotenv.config();

// ---- App setup -------------------------------------------------------------
const app = express();
const PORT = process.env.PORT || 3000;
const MONGODB_URI = process.env.MONGODB_URI || "mongodb://localhost:27017/statsmate";

// ---- Middleware ------------------------------------------------------------
app.use(cors());
app.use(express.json());
app.use(express.urlencoded({ extended: true }));
app.use(morgan("dev")); // Request logging

// ---- Database connection ---------------------------------------------------
mongoose
  .connect(MONGODB_URI)
  .then(() => console.log("âœ… MongoDB connected successfully"))
  .catch((err) => {
    console.error("âŒ MongoDB connection error:", err);
    process.exit(1);
  });

// ---- Routes ----------------------------------------------------------------
app.use("/api/auth", authRoutes);
app.use("/api", fileRoutes);
app.use("/api", analysisRoutes);
app.use("/api", resultRoutes);
app.use("/api", glossaryRoutes);
app.use("/api", healthRoutes);

// ---- Root endpoint ---------------------------------------------------------
app.get("/", (req, res) => {
  res.json({
    message: "StatsMate Core API",
    version: "1.0.0",
    endpoints: {
      auth: "/api/auth",
      files: "/api/files",
      upload: "/api/upload",
      analyze: "/api/analyze",
      results: "/api/result/:id",
      glossary: "/api/glossary/:term",
      health: "/api/health"
    }
  });
});

// ---- Error handler (must be LAST) -----------------------------------------
app.use(errorHandler);

// ---- Start server ----------------------------------------------------------
app.listen(PORT, "0.0.0.0", () => {
  console.log(`ðŸš€ Core API running on http://0.0.0.0:${PORT}`);
  console.log(`ðŸ“Š Environment: ${process.env.NODE_ENV || "development"}`);
});

================================================================================
FILE: backend/models/User.js
================================================================================

// models/User.js
// User schema for authentication and data isolation

import mongoose from "mongoose";
import bcrypt from "bcrypt";

const userSchema = new mongoose.Schema(
  {
    email: {
      type: String,
      required: [true, "Email is required"],
      unique: true,
      lowercase: true,
      trim: true,
      match: [/^\S+@\S+\.\S+$/, "Please provide a valid email"],
    },
    password: {
      type: String,
      required: [true, "Password is required"],
      minlength: [6, "Password must be at least 6 characters"],
    },
    name: {
      type: String,
      trim: true,
    },
  },
  {
    timestamps: true,
  }
);

// Hash password before saving
userSchema.pre("save", async function (next) {
  // Only hash if password is modified
  if (!this.isModified("password")) return next();
  
  try {
    const salt = await bcrypt.genSalt(10);
    this.password = await bcrypt.hash(this.password, salt);
    next();
  } catch (error) {
    next(error);
  }
});

// Method to compare passwords
userSchema.methods.comparePassword = async function (candidatePassword) {
  return await bcrypt.compare(candidatePassword, this.password);
};

// Remove password from JSON responses
userSchema.methods.toJSON = function () {
  const obj = this.toObject();
  delete obj.password;
  return obj;
};

export default mongoose.model("User", userSchema);

================================================================================
FILE: backend/models/File.js
================================================================================

// models/File.js
// File schema for uploaded datasets (CSV/XLSX)

import mongoose from "mongoose";

const fileSchema = new mongoose.Schema(
  {
    userId: {
      type: mongoose.Schema.Types.ObjectId,
      ref: "User",
      required: true,
      index: true, // Performance optimization for userId queries
    },
    filename: {
      type: String,
      required: true,
    },
    originalName: {
      type: String,
      required: true,
    },
    path: {
      type: String,
      required: true,
    },
    size: {
      type: Number,
      required: true,
    },
    mimetype: {
      type: String,
      required: true,
      enum: ["text/csv", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"],
    },
    preview: {
      columns: [String],
      rows: [[mongoose.Schema.Types.Mixed]],
      detectedTypes: {
        type: Map,
        of: String,
      },
    },
    status: {
      type: String,
      enum: ["active", "deleted"],
      default: "active",
    },
  },
  {
    timestamps: true,
  }
);

// Index for efficient queries
fileSchema.index({ userId: 1, status: 1 });

export default mongoose.model("File", fileSchema);

================================================================================
FILE: backend/models/Analysis.js
================================================================================

// models/Analysis.js
// Analysis schema for storing analysis configurations and results

import mongoose from "mongoose";

const analysisSchema = new mongoose.Schema(
  {
    userId: {
      type: mongoose.Schema.Types.ObjectId,
      ref: "User",
      required: true,
      index: true,
    },
    fileId: {
      type: mongoose.Schema.Types.ObjectId,
      ref: "File",
      required: true,
    },
    dependentVar: {
      type: String,
      required: true,
    },
    independentVars: {
      type: [String],
      required: true,
      validate: {
        validator: function (v) {
          return v && v.length > 0;
        },
        message: "At least one independent variable is required",
      },
    },
    modelType: {
      type: String,
      enum: ["ols", "logistic", "auto"],
      default: "auto",
    },
    status: {
      type: String,
      enum: ["pending", "processing", "completed", "failed", "queued"],
      default: "pending",
    },
    queuePosition: {
      type: Number,
      default: null,
    },
    // Results from R service (stored after completion)
    results: {
      model_type: String,
      coefficients: mongoose.Schema.Types.Mixed,
      statistics: mongoose.Schema.Types.Mixed,
      diagnostics: mongoose.Schema.Types.Mixed,
      predictions: mongoose.Schema.Types.Mixed,
    },
    // AI-generated interpretation
    interpretation: {
      type: String,
      default: null,
    },
    // Error information (if failed)
    error: {
      message: String,
      code: String,
      timestamp: Date,
    },
    // Processing metrics
    processingTime: {
      type: Number, // milliseconds
      default: null,
    },
    completedAt: {
      type: Date,
      default: null,
    },
  },
  {
    timestamps: true,
  }
);

// Indexes for efficient queries
analysisSchema.index({ userId: 1, status: 1 });
analysisSchema.index({ userId: 1, createdAt: -1 });

export default mongoose.model("Analysis", analysisSchema);

================================================================================
FILE: backend/middleware/auth.js
================================================================================

// middleware/auth.js
// JWT authentication middleware for protecting routes

import jwt from "jsonwebtoken";

const JWT_SECRET = process.env.JWT_SECRET || "your-secret-key-change-in-production";

const authenticateToken = (req, res, next) => {
  // Get token from Authorization header
  const authHeader = req.headers["authorization"];
  const token = authHeader && authHeader.split(" ")[1]; // Format: "Bearer TOKEN"

  if (!token) {
    return res.status(401).json({
      error: "UNAUTHORIZED",
      message: "Access token is required",
    });
  }

  try {
    // Verify token
    const decoded = jwt.verify(token, JWT_SECRET);
    
    // Attach user info to request
    req.user = {
      id: decoded.userId,
      email: decoded.email,
    };
    
    next();
  } catch (error) {
    if (error.name === "TokenExpiredError") {
      return res.status(401).json({
        error: "TOKEN_EXPIRED",
        message: "Access token has expired",
      });
    }
    
    return res.status(403).json({
      error: "INVALID_TOKEN",
      message: "Invalid access token",
    });
  }
};

export default authenticateToken;

================================================================================
FILE: backend/middleware/errorHandler.js
================================================================================

// middleware/errorHandler.js
// Centralized error handling middleware

const errorHandler = (err, req, res, next) => {
  console.error("âŒ Error:", err);

  // Mongoose validation error
  if (err.name === "ValidationError") {
    const errors = Object.values(err.errors).map((e) => e.message);
    return res.status(400).json({
      error: "VALIDATION_ERROR",
      message: "Validation failed",
      details: errors,
    });
  }

  // Mongoose duplicate key error
  if (err.code === 11000) {
    const field = Object.keys(err.keyPattern)[0];
    return res.status(409).json({
      error: "DUPLICATE_ERROR",
      message: `${field} already exists`,
    });
  }

  // Mongoose cast error (invalid ObjectId)
  if (err.name === "CastError") {
    return res.status(400).json({
      error: "INVALID_ID",
      message: "Invalid resource ID",
    });
  }

  // Multer file upload errors
  if (err.name === "MulterError") {
    if (err.code === "LIMIT_FILE_SIZE") {
      return res.status(400).json({
        error: "FILE_TOO_LARGE",
        message: "File size exceeds 5MB limit",
      });
    }
    return res.status(400).json({
      error: "UPLOAD_ERROR",
      message: err.message,
    });
  }

  // JWT errors (should be caught by auth middleware, but just in case)
  if (err.name === "JsonWebTokenError") {
    return res.status(403).json({
      error: "INVALID_TOKEN",
      message: "Invalid token",
    });
  }

  if (err.name === "TokenExpiredError") {
    return res.status(401).json({
      error: "TOKEN_EXPIRED",
      message: "Token has expired",
    });
  }

  // Custom application errors
  if (err.status) {
    return res.status(err.status).json({
      error: err.code || "APPLICATION_ERROR",
      message: err.message,
    });
  }

  // Default internal server error
  res.status(500).json({
    error: "INTERNAL_SERVER_ERROR",
    message: "An unexpected error occurred",
    ...(process.env.NODE_ENV === "development" && { details: err.message }),
  });
};

export default errorHandler;

================================================================================
FILE: backend/middleware/upload.js
================================================================================

// middleware/upload.js
// Multer configuration for file uploads (CSV/XLSX only, 5MB max)

import multer from "multer";
import path from "path";
import fs from "fs";

// Ensure uploads directory exists
const uploadsDir = path.join(process.cwd(), "uploads");
if (!fs.existsSync(uploadsDir)) {
  fs.mkdirSync(uploadsDir, { recursive: true });
}

// Configure storage
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, uploadsDir);
  },
  filename: (req, file, cb) => {
    // Generate unique filename: timestamp-random-originalname
    const uniqueSuffix = `${Date.now()}-${Math.round(Math.random() * 1e9)}`;
    const ext = path.extname(file.originalname);
    const basename = path.basename(file.originalname, ext);
    cb(null, `${uniqueSuffix}-${basename}${ext}`);
  },
});

// File filter - only allow CSV and XLSX
const fileFilter = (req, file, cb) => {
  const allowedMimes = [
    "text/csv",
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
  ];
  
  const ext = path.extname(file.originalname).toLowerCase();
  const allowedExts = [".csv", ".xlsx"];

  if (allowedMimes.includes(file.mimetype) && allowedExts.includes(ext)) {
    cb(null, true);
  } else {
    cb(
      new Error("Invalid file type. Only CSV and XLSX files are allowed."),
      false
    );
  }
};

// Configure multer
const upload = multer({
  storage,
  fileFilter,
  limits: {
    fileSize: 5 * 1024 * 1024, // 5MB limit
  },
});

export default upload;

================================================================================
FILE: backend/routes/authRoutes.js
================================================================================

// routes/authRoutes.js
// Authentication endpoints: register and login

import express from "express";
import jwt from "jsonwebtoken";
import User from "../models/User.js";

const router = express.Router();
const JWT_SECRET = process.env.JWT_SECRET || "your-secret-key-change-in-production";
const JWT_EXPIRES_IN = process.env.JWT_EXPIRES_IN || "7d";

// POST /api/auth/register
router.post("/register", async (req, res, next) => {
  try {
    const { email, password, name } = req.body;

    // Validation
    if (!email || !password) {
      return res.status(400).json({
        error: "VALIDATION_ERROR",
        message: "Email and password are required",
      });
    }

    // Check if user already exists
    const existingUser = await User.findOne({ email });
    if (existingUser) {
      return res.status(409).json({
        error: "USER_EXISTS",
        message: "User with this email already exists",
      });
    }

    // Create new user
    const user = new User({
      email,
      password,
      name,
    });

    await user.save();

    // Generate JWT token
    const token = jwt.sign(
      {
        userId: user._id,
        email: user.email,
      },
      JWT_SECRET,
      { expiresIn: JWT_EXPIRES_IN }
    );

    res.status(201).json({
      message: "User registered successfully",
      token,
      user: {
        id: user._id,
        email: user.email,
        name: user.name,
      },
    });
  } catch (error) {
    next(error);
  }
});

// POST /api/auth/login
router.post("/login", async (req, res, next) => {
  try {
    const { email, password } = req.body;

    // Validation
    if (!email || !password) {
      return res.status(400).json({
        error: "VALIDATION_ERROR",
        message: "Email and password are required",
      });
    }

    // Find user
    const user = await User.findOne({ email });
    if (!user) {
      return res.status(401).json({
        error: "INVALID_CREDENTIALS",
        message: "Invalid email or password",
      });
    }

    // Check password
    const isMatch = await user.comparePassword(password);
    if (!isMatch) {
      return res.status(401).json({
        error: "INVALID_CREDENTIALS",
        message: "Invalid email or password",
      });
    }

    // Generate JWT token
    const token = jwt.sign(
      {
        userId: user._id,
        email: user.email,
      },
      JWT_SECRET,
      { expiresIn: JWT_EXPIRES_IN }
    );

    res.json({
      message: "Login successful",
      token,
      user: {
        id: user._id,
        email: user.email,
        name: user.name,
      },
    });
  } catch (error) {
    next(error);
  }
});

export default router;

================================================================================
FILE: backend/routes/fileRoutes.js
================================================================================

// routes/fileRoutes.js
// File management endpoints: upload, list, get, delete

import express from "express";
import fs from "fs/promises";
import path from "path";
import Papa from "papaparse";
import XLSX from "xlsx";
import authenticateToken from "../middleware/auth.js";
import upload from "../middleware/upload.js";
import File from "../models/File.js";

const router = express.Router();

// POST /api/upload - Upload a new file
router.post("/upload", authenticateToken, upload.single("file"), async (req, res, next) => {
  try {
    if (!req.file) {
      return res.status(400).json({
        error: "NO_FILE",
        message: "No file uploaded",
      });
    }

    // Generate preview based on file type
    const preview = await generatePreview(req.file.path, req.file.mimetype);

    // Create file record in database
    const file = new File({
      userId: req.user.id,
      filename: req.file.filename,
      originalName: req.file.originalname,
      path: req.file.path,
      size: req.file.size,
      mimetype: req.file.mimetype,
      preview,
    });

    await file.save();

    res.status(201).json({
      message: "File uploaded successfully",
      file: {
        id: file._id,
        filename: file.filename,
        originalName: file.originalName,
        size: file.size,
        mimetype: file.mimetype,
        preview: file.preview,
        uploadedAt: file.createdAt,
      },
    });
  } catch (error) {
    // Clean up uploaded file if database save fails
    if (req.file) {
      await fs.unlink(req.file.path).catch(() => {});
    }
    next(error);
  }
});

// GET /api/files - List all files for the authenticated user
router.get("/files", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Only fetch files belonging to the authenticated user
    const files = await File.find({
      userId: req.user.id,
      status: "active",
    }).sort({ createdAt: -1 });

    const fileList = files.map((file) => ({
      id: file._id,
      filename: file.filename,
      originalName: file.originalName,
      size: file.size,
      mimetype: file.mimetype,
      uploadedAt: file.createdAt,
      preview: file.preview,
    }));

    res.json({
      count: fileList.length,
      files: fileList,
    });
  } catch (error) {
    next(error);
  }
});

// GET /api/files/:id - Get specific file details
router.get("/files/:id", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Verify ownership
    const file = await File.findOne({
      _id: req.params.id,
      userId: req.user.id,
      status: "active",
    });

    if (!file) {
      return res.status(404).json({
        error: "FILE_NOT_FOUND",
        message: "File not found or access denied",
      });
    }

    res.json({
      id: file._id,
      filename: file.filename,
      originalName: file.originalName,
      size: file.size,
      mimetype: file.mimetype,
      uploadedAt: file.createdAt,
      preview: file.preview,
    });
  } catch (error) {
    next(error);
  }
});

// DELETE /api/files/:id - Delete a file
router.delete("/files/:id", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Verify ownership
    const file = await File.findOne({
      _id: req.params.id,
      userId: req.user.id,
      status: "active",
    });

    if (!file) {
      return res.status(404).json({
        error: "FILE_NOT_FOUND",
        message: "File not found or access denied",
      });
    }

    // Mark as deleted (soft delete)
    file.status = "deleted";
    await file.save();

    // Delete physical file
    await fs.unlink(file.path).catch((err) => {
      console.error("Failed to delete physical file:", err);
    });

    res.json({
      message: "File deleted successfully",
      id: file._id,
    });
  } catch (error) {
    next(error);
  }
});

// Helper function to generate preview
async function generatePreview(filePath, mimetype) {
  try {
    if (mimetype === "text/csv") {
      // Parse CSV
      const content = await fs.readFile(filePath, "utf-8");
      const result = Papa.parse(content, { header: false });
      
      // Get first 6 rows (header + 5 data rows)
      const rows = result.data.slice(0, 6);
      const columns = rows[0] || [];
      
      // Detect types for each column
      const detectedTypes = {};
      if (rows.length > 1) {
        columns.forEach((col, idx) => {
          const values = rows.slice(1).map((row) => row[idx]).filter(Boolean);
          detectedTypes[col] = detectColumnType(values);
        });
      }

      return {
        columns,
        rows: rows.slice(0, 6),
        detectedTypes,
      };
    } else if (
      mimetype === "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    ) {
      // Parse XLSX
      const workbook = XLSX.readFile(filePath);
      const sheetName = workbook.SheetNames[0];
      const sheet = workbook.Sheets[sheetName];
      const data = XLSX.utils.sheet_to_json(sheet, { header: 1 });
      
      // Get first 6 rows
      const rows = data.slice(0, 6);
      const columns = rows[0] || [];
      
      // Detect types
      const detectedTypes = {};
      if (rows.length > 1) {
        columns.forEach((col, idx) => {
          const values = rows.slice(1).map((row) => row[idx]).filter(val => val !== undefined && val !== null);
          detectedTypes[col] = detectColumnType(values);
        });
      }

      return {
        columns,
        rows,
        detectedTypes,
      };
    }

    return null;
  } catch (error) {
    console.error("Error generating preview:", error);
    return null;
  }
}

// Helper function to detect column type
function detectColumnType(values) {
  if (values.length === 0) return "unknown";

  // Check if all values are numeric
  const numericValues = values.filter((v) => !isNaN(parseFloat(v)));
  if (numericValues.length === values.length) {
    // Check if binary (only 0 and 1)
    const uniqueValues = [...new Set(values.map((v) => parseFloat(v)))];
    if (uniqueValues.length <= 2 && uniqueValues.every((v) => v === 0 || v === 1)) {
      return "binary";
    }
    return "continuous";
  }

  // Check if categorical
  const uniqueValues = new Set(values);
  if (uniqueValues.size < values.length * 0.5) {
    return "categorical";
  }

  return "text";
}

export default router;

================================================================================
FILE: backend/routes/analysisRoutes.js
================================================================================

// routes/analysisRoutes.js
// Analysis workflow: receive request, forward to Amanda's Integration API

import express from "express";
import axios from "axios";
import authenticateToken from "../middleware/auth.js";
import File from "../models/File.js";
import Analysis from "../models/Analysis.js";

const router = express.Router();

const INTEGRATION_API_URL =
  process.env.INTEGRATION_API_URL || "http://localhost:5001";

// POST /api/analyze - Start new analysis
router.post("/analyze", authenticateToken, async (req, res, next) => {
  try {
    const { fileId, dependentVar, independentVars, modelType } = req.body;

    // Validation
    if (!fileId || !dependentVar || !independentVars || independentVars.length === 0) {
      return res.status(400).json({
        error: "VALIDATION_ERROR",
        message: "fileId, dependentVar, and independentVars are required",
      });
    }

    // CRITICAL: Verify file ownership
    const file = await File.findOne({
      _id: fileId,
      userId: req.user.id,
      status: "active",
    });

    if (!file) {
      return res.status(404).json({
        error: "FILE_NOT_FOUND",
        message: "File not found or access denied",
      });
    }

    // Create analysis record (status: pending)
    const analysis = new Analysis({
      userId: req.user.id,
      fileId: file._id,
      dependentVar,
      independentVars,
      modelType: modelType || "auto",
      status: "pending",
    });

    await analysis.save();

    // Forward to Integration API (Amanda)
    try {
      const startTime = Date.now();
      
      const response = await axios.post(
        `${INTEGRATION_API_URL}/integrate/analyze`,
        {
          analysisId: analysis._id.toString(),
          userId: req.user.id,
          data_path: file.path,
          dependentVar,
          independentVars,
          modelType: modelType || "auto",
        },
        {
          timeout: 60000, // 60 second timeout
        }
      );

      const processingTime = Date.now() - startTime;

      // Check if queued or immediate processing
      if (response.data.status === "queued") {
        // Request was queued
        analysis.status = "queued";
        analysis.queuePosition = response.data.queuePosition;
        await analysis.save();

        return res.status(202).json({
          message: "Analysis queued",
          analysisId: analysis._id,
          status: "queued",
          queuePosition: response.data.queuePosition,
          estimatedWait: response.data.estimatedWait,
        });
      } else if (response.data.status === "completed") {
        // Immediate processing completed
        analysis.status = "completed";
        analysis.results = response.data.results;
        analysis.interpretation = response.data.interpretation;
        analysis.processingTime = processingTime;
        analysis.completedAt = new Date();
        await analysis.save();

        return res.status(200).json({
          message: "Analysis completed",
          analysisId: analysis._id,
          status: "completed",
          results: response.data.results,
          interpretation: response.data.interpretation,
          processingTime,
        });
      }
    } catch (integrationError) {
      // Handle errors from Integration API
      if (integrationError.response?.status === 429) {
        // Queue is full
        analysis.status = "failed";
        analysis.error = {
          message: "System is at capacity. Please try again later.",
          code: "QUEUE_FULL",
          timestamp: new Date(),
        };
        await analysis.save();

        return res.status(429).json({
          error: "QUEUE_FULL",
          message: "System is at capacity. Please try again later.",
          analysisId: analysis._id,
        });
      }

      // Other integration errors
      analysis.status = "failed";
      analysis.error = {
        message:
          integrationError.response?.data?.message ||
          integrationError.message ||
          "Integration service error",
        code: "INTEGRATION_ERROR",
        timestamp: new Date(),
      };
      await analysis.save();

      return res.status(500).json({
        error: "INTEGRATION_ERROR",
        message: "Failed to process analysis request",
        analysisId: analysis._id,
      });
    }
  } catch (error) {
    next(error);
  }
});

// GET /api/analyze/:id/status - Check analysis status
router.get("/analyze/:id/status", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Verify ownership
    const analysis = await Analysis.findOne({
      _id: req.params.id,
      userId: req.user.id,
    });

    if (!analysis) {
      return res.status(404).json({
        error: "ANALYSIS_NOT_FOUND",
        message: "Analysis not found or access denied",
      });
    }

    res.json({
      analysisId: analysis._id,
      status: analysis.status,
      queuePosition: analysis.queuePosition,
      processingTime: analysis.processingTime,
      completedAt: analysis.completedAt,
      error: analysis.error,
    });
  } catch (error) {
    next(error);
  }
});

export default router;

================================================================================
FILE: backend/routes/resultRoutes.js
================================================================================

// routes/resultRoutes.js
// Result retrieval endpoints

import express from "express";
import authenticateToken from "../middleware/auth.js";
import Analysis from "../models/Analysis.js";
import File from "../models/File.js";

const router = express.Router();

// GET /api/result/:id - Get analysis results
router.get("/result/:id", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Verify ownership
    const analysis = await Analysis.findOne({
      _id: req.params.id,
      userId: req.user.id,
    }).populate("fileId");

    if (!analysis) {
      return res.status(404).json({
        error: "ANALYSIS_NOT_FOUND",
        message: "Analysis not found or access denied",
      });
    }

    // Return different responses based on status
    if (analysis.status === "pending" || analysis.status === "processing") {
      return res.status(202).json({
        analysisId: analysis._id,
        status: analysis.status,
        message: "Analysis is still processing",
      });
    }

    if (analysis.status === "queued") {
      return res.status(202).json({
        analysisId: analysis._id,
        status: "queued",
        queuePosition: analysis.queuePosition,
        message: "Analysis is queued",
      });
    }

    if (analysis.status === "failed") {
      return res.status(500).json({
        analysisId: analysis._id,
        status: "failed",
        error: analysis.error,
        message: "Analysis failed",
      });
    }

    // Status is "completed"
    res.json({
      analysisId: analysis._id,
      status: "completed",
      file: {
        id: analysis.fileId._id,
        filename: analysis.fileId.filename,
        originalName: analysis.fileId.originalName,
      },
      configuration: {
        dependentVar: analysis.dependentVar,
        independentVars: analysis.independentVars,
        modelType: analysis.modelType,
      },
      results: analysis.results,
      interpretation: analysis.interpretation,
      processingTime: analysis.processingTime,
      completedAt: analysis.completedAt,
      createdAt: analysis.createdAt,
    });
  } catch (error) {
    next(error);
  }
});

// GET /api/results - List all analyses for user
router.get("/results", authenticateToken, async (req, res, next) => {
  try {
    // CRITICAL: Only fetch analyses belonging to the authenticated user
    const analyses = await Analysis.find({
      userId: req.user.id,
    })
      .populate("fileId")
      .sort({ createdAt: -1 })
      .limit(50); // Limit to last 50 analyses

    const analysisList = analyses.map((analysis) => ({
      id: analysis._id,
      status: analysis.status,
      file: {
        id: analysis.fileId?._id,
        filename: analysis.fileId?.filename,
        originalName: analysis.fileId?.originalName,
      },
      configuration: {
        dependentVar: analysis.dependentVar,
        independentVars: analysis.independentVars,
        modelType: analysis.modelType,
      },
      processingTime: analysis.processingTime,
      completedAt: analysis.completedAt,
      createdAt: analysis.createdAt,
    }));

    res.json({
      count: analysisList.length,
      analyses: analysisList,
    });
  } catch (error) {
    next(error);
  }
});

export default router;

================================================================================
FILE: backend/routes/glossaryRoutes.js
================================================================================

// routes/glossaryRoutes.js
// Glossary endpoints for statistical term definitions (Darwin's work)

import express from "express";
import path from "path";
import fs from "fs/promises";

const router = express.Router();

// Path to glossary JSON file (provided by Darwin)
const GLOSSARY_PATH = path.join(process.cwd(), "glossary.json");

// Cache glossary in memory
let glossaryCache = null;

// Load glossary on startup
async function loadGlossary() {
  try {
    const data = await fs.readFile(GLOSSARY_PATH, "utf-8");
    glossaryCache = JSON.parse(data);
    console.log(`âœ… Loaded ${Object.keys(glossaryCache).length} glossary terms`);
  } catch (error) {
    console.warn("âš ï¸  Glossary file not found, using empty glossary");
    glossaryCache = {};
  }
}

// Load glossary immediately
loadGlossary();

// GET /api/glossary/:term - Get definition for a specific term
router.get("/glossary/:term", (req, res) => {
  const term = req.params.term.toLowerCase();

  if (!glossaryCache) {
    return res.status(503).json({
      error: "GLOSSARY_UNAVAILABLE",
      message: "Glossary is not available",
    });
  }

  const definition = glossaryCache[term];

  if (!definition) {
    return res.status(404).json({
      error: "TERM_NOT_FOUND",
      message: `Term '${req.params.term}' not found in glossary`,
    });
  }

  res.json({
    term: req.params.term,
    definition,
  });
});

// GET /api/glossary - Get all glossary terms
router.get("/glossary", (req, res) => {
  if (!glossaryCache) {
    return res.status(503).json({
      error: "GLOSSARY_UNAVAILABLE",
      message: "Glossary is not available",
    });
  }

  res.json({
    terms: Object.keys(glossaryCache).length,
    glossary: glossaryCache,
  });
});

export default router;

================================================================================
FILE: backend/routes/healthRoutes.js
================================================================================

// routes/healthRoutes.js
// Health check endpoints for monitoring

import express from "express";
import mongoose from "mongoose";
import axios from "axios";

const router = express.Router();

const INTEGRATION_API_URL =
  process.env.INTEGRATION_API_URL || "http://localhost:5001";

// GET /api/health - Basic health check
router.get("/health", (req, res) => {
  const health = {
    status: "ok",
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    database: mongoose.connection.readyState === 1 ? "connected" : "disconnected",
  };

  const statusCode = health.database === "connected" ? 200 : 503;
  res.status(statusCode).json(health);
});

// GET /api/health/detailed - Detailed health check including dependencies
router.get("/health/detailed", async (req, res) => {
  const health = {
    status: "ok",
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    services: {
      api: "ok",
      database: mongoose.connection.readyState === 1 ? "ok" : "error",
      integration: "unknown",
    },
  };

  // Check Integration API
  try {
    await axios.get(`${INTEGRATION_API_URL}/integrate/health`, {
      timeout: 3000,
    });
    health.services.integration = "ok";
  } catch (error) {
    health.services.integration = "error";
    health.status = "degraded";
  }

  // Determine overall status
  if (health.services.database === "error") {
    health.status = "error";
  } else if (health.services.integration === "error") {
    health.status = "degraded";
  }

  const statusCode = health.status === "ok" ? 200 : health.status === "degraded" ? 200 : 503;
  res.status(statusCode).json(health);
});

export default router;

================================================================================
FILE: backend/package.json
================================================================================

{
  "name": "statsmate-core-api",
  "version": "1.0.0",
  "description": "StatsMate Core API - Main entry point for authentication, file management, and analysis orchestration",
  "type": "module",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [
    "statsmate",
    "api",
    "statistics",
    "regression"
  ],
  "author": "Khalil - Core API Lead",
  "license": "MIT",
  "dependencies": {
    "express": "^5.0.1",
    "mongoose": "^8.8.4",
    "bcrypt": "^5.1.1",
    "jsonwebtoken": "^9.0.2",
    "multer": "^1.4.5-lts.1",
    "papaparse": "^5.4.1",
    "xlsx": "^0.18.5",
    "axios": "^1.7.9",
    "cors": "^2.8.5",
    "dotenv": "^16.4.7",
    "morgan": "^1.10.0"
  },
  "devDependencies": {
    "nodemon": "^3.1.9"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}

================================================================================
FILE: backend/.env.example
================================================================================

# Core API Configuration
PORT=3000
NODE_ENV=development

# MongoDB Configuration
MONGODB_URI=mongodb://localhost:27017/statsmate

# JWT Configuration
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production
JWT_EXPIRES_IN=7d

# Integration API Configuration
INTEGRATION_API_URL=http://localhost:5001

# CORS Configuration (optional)
CORS_ORIGIN=http://localhost:5173

================================================================================
FILE: backend/.gitignore
================================================================================

# Dependencies
node_modules/
package-lock.json

# Environment variables
.env
.env.local
.env.*.local

# Uploads directory (keep structure, ignore files)
uploads/*
!uploads/.gitkeep

# Logs
*.log
npm-debug.log*
access.log
error.log

# OS files
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Testing
coverage/
.nyc_output/

# Build outputs
dist/
build/

# Temporary files
tmp/
temp/
*.tmp

================================================================================
FILE: backend/Dockerfile
================================================================================

# Dockerfile for StatsMate Core API
FROM node:18-alpine

# Set working directory
WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm install --production

# Copy application code
COPY . .

# Create uploads directory
RUN mkdir -p uploads

# Expose port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"

# Start the application
CMD ["node", "server.js"]

================================================================================
FILE: backend/.dockerignore
================================================================================

node_modules
npm-debug.log
.env
.env.local
.git
.gitignore
README.md
uploads/*
!uploads/.gitkeep
*.log
.DS_Store
coverage
.vscode
.idea

================================================================================
FILE: backend/uploads/.gitkeep
================================================================================

# This file keeps the uploads directory in version control
# The directory is needed for file uploads, but actual files are gitignored

================================================================================
SETUP INSTRUCTIONS
================================================================================

1. CREATE DIRECTORY STRUCTURE:
   
   cd "C:\Users\Owner\Documents\CSC 230 Final Code"
   mkdir backend
   cd backend
   mkdir models routes middleware uploads

2. COPY FILES:
   
   Copy each section above into its corresponding file path.
   Use the "FILE: backend/..." headers to know where each section goes.

3. INSTALL DEPENDENCIES:
   
   npm install

4. CREATE .ENV FILE:
   
   Copy .env.example to .env:
   cp .env.example .env
   
   Then edit .env with your settings:
   - Change JWT_SECRET to a random string
   - Update MONGODB_URI if needed
   - Update INTEGRATION_API_URL if needed

5. START MONGODB:
   
   docker run -d -p 27017:27017 --name mongodb mongo:latest

6. START THE SERVER:
   
   npm run dev

7. TEST:
   
   # Health check
   curl http://localhost:3000/api/health
   
   # Register user
   curl -X POST http://localhost:3000/api/auth/register ^
     -H "Content-Type: application/json" ^
     -d "{\"email\":\"test@example.com\",\"password\":\"password123\"}"

================================================================================
QUICK REFERENCE
================================================================================

API Endpoints:
- POST /api/auth/register     - Register new user
- POST /api/auth/login        - Login user
- POST /api/upload            - Upload file (requires auth)
- GET  /api/files             - List files (requires auth)
- GET  /api/files/:id         - Get file (requires auth)
- DELETE /api/files/:id       - Delete file (requires auth)
- POST /api/analyze           - Start analysis (requires auth)
- GET  /api/analyze/:id/status - Check status (requires auth)
- GET  /api/result/:id        - Get results (requires auth)
- GET  /api/results           - List analyses (requires auth)
- GET  /api/glossary/:term    - Get term definition
- GET  /api/health            - Health check

Ports:
- Core API: 3000
- MongoDB: 27017
- Integration API: 5001 (Amanda's work)
- Frontend: 5173 (Dhwani's work)

Environment Variables:
- PORT=3000
- MONGODB_URI=mongodb://localhost:27017/statsmate
- JWT_SECRET=your-secret-key
- INTEGRATION_API_URL=http://localhost:5001

================================================================================
END OF FILE
================================================================================